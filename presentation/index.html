<!DOCTYPE html>
<html>
  <head>
    <title>Understanding Spark</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Understanding Spark
### by Arun Allamsetty

---

# Agenda

1. Big Data
2. Spark vs Hadoop
3.

---

# Big Data

## What comes to mind?
--

- Yet another buzzword, maybe?

--

- Hadoop?

---

# Big Data

## What comes to mind?

- Yet another buzzword, maybe?

- ~~Hadoop?~~ Probably not.

--

- Spark

--

Why?

---

# Let's start with Hadoop
--

### The Good
--

- Distributed computing was hard and expensive.

--

- MapReduce paradigm based on a paper published by Google.

???
Any computation performed by a functional map and reduce can be easily parallelized.
--

- Fault tolerant distributed file system: HDFS

--

- Inexpensive to operate as it could run on commodity hardware.

--

### The bad

--

- Writes data to disk after each operation.

???
This was done to provide a checkpoint feature.
--

- Doesn't support in-memory operations.

--

- While MapReduce is easier, it is no SQL (pun not intended).

---

# What about Spark?

???
Takes all the good things from Hadoop and leaves out the bad.
--

- Supports HDFS (or other Hadoop-supported file systems). For example,

--

  - Amazon S3
--

  - Windows Azure Storage Blobs (WASB).

--

- *Does not* write to disk after each operation.
--

  - Uses RDD (Resilient Distributed Datasets) which can recover from failures.
--

  - Is a lot faster than Hadoop.

--

- Supports in-memory operations.

???
Is one of its major selling points.
--

- Written in Scala. Also supports Java and Python.

???
But we will mainly talk about the Scala API.
---

# RDD (Resilient Distributed Datasets)

--

- Based on a paper by Matei Zaharia.

  - _Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing._

???
Berkeley AMPLab alum and co-founder of Databricks which is primary Spark contributer.
--

They are,

--

- Distributed: Data resides on different JVMs on different nodes.

--

- Typed: Data in RDDs can be assigned types.
  - For eg. `RDD[String]`, `RDD[Long]`, `RDD[Person]`, etc.

--

- Immutable: Data in an RDD, cannot be changed but can only be transformed, resulting in a new RDD.

???
Resulting in a new RDD.
--

- Lazily evaluated: No operation is performed on an RDD until an action, which forces execution, is performed.

--

- Fault-tolerant: Can recover damaged partitions.

--

- Supports a lot more oprations than `map` and `reduce`.

???
Let's talk about these more now.
---

# Creating an RDD

--

- Can be created using a `SparkContext`.

--

  - A `SparkContext` is an entry point into Spark for any Spark application.

--

```scala
import org.apache.spark.SparkContext

object LearningRdd {
  // Parameters in Scala can be named.
  val sc = new SparkContext(master = "master[2]", appName = "learningRdd")

  // They can be partially named.
  // `numSlices` defaults to `sc.defaultParallelism` which maps to total
  // number of cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)

  // They don't have to be named.
  val rddFromTextFile = sc.textFile("/path/to/text/file")
}
```

--

- At this point none of these `RDD`s have even been computed. We would have to perform an action which forces execution.

--

```scala
val sum = rddFromCollection.reduce(_ + _) // same as `reduce { (x, y) => x + y }`
```

---

# Why are RDDs lazy?

--

Let's look at this code snippet.

--

```scala
val nums = sc.parallelize(1 to 100)

// `groupBy` returns an RDD of key-value pairs, `RDD[(Int, Iterable[Int])]` here.
val grouped = nums.groupBy(_ % 3)

// `_1` accesses the first element in a Scala tuple.
// `max` forces computation of an RDD like `reduce`.
val maxKey = grouped.map(_._1).max

// `flatMap` maps over any iterable and flattens all the elements.
val sum = grouped
  .flatMap(_._2)
  .map(_ * maxKey)
  .reduce(_ + _)
```

--

In the Spark UI, we'll see this:

--

.center[![:scale 100%](images/skipped.png)]

---

# RDD dependency (or lineage) graph

--

Spark first creates a DAG of all the operations to be performed. The DAG for the previous `maxKey` looks like this.

--

.center[![:scale 40%](images/maxDag.png)]

---

# RDD dependency (or lineage) graph

The DAG for `sum` however looks like this.

--

.center[![:scale 40%](images/sumDag.png)]

--

Hence, lazy evaluation allows Spark avoid unnecessary recomputation of the same data.

---

# (R)esilience in RDD

--

With this information, let's see how RDDs are fault-tolerant.

--

- If at a stage, a partition is lost due to some reason, Spark knows exactly how to get a new partition at that stage due to the DAG it created in the first place.

--

- Immutability of RDDs ensures that the newly resurrected partition has exactly the same data as before.

---

# RDDs are typed

--

Here's an example.

--

```scala
// Case classes in Scala are a classes with a few helper functions prebuilt
// into them to avoid boilerplate code like in Java.
case class Person(fname: String, lname: String, age: Int)

// :: is a function in a Scala `List` which lets you prepend to a list. `Nil`
// is an empty list. One of the helper function in a case class is `apply`
// which lets you construct objects with function-like syntax.
// `Person("John", "Doe", 25)` is same as `Person.apply("John", "Doe", 25)`.
val persons = sc.parallelize(
  Person("John", "Doe", 38) ::
  Person("Jane", "Doe", 35) ::
  Person("Jon", "Doe", 20) ::
  Persom("Jonathan", "Doe", 31) :: Nil)

val oldPersons = persons.filter(_.age > 30)
val oldJons = oldPersons.filter(_.name.toLowerCase.startsWith("jon"))
val numOldJons = oldJos.count
```

---

# SQL on Spark

--

SQL has been the de facto standard for data analysis since a long time.

--

  - Hadoop lacked in this area. SQL users were not excited to use MapReduce.

--

  - Hive was created to run SQL on Hadoop.

--

  - It was very popular, but was painfully slow.

--

To cater to SQL users, Spark added support for HiveQL which was dubbed Shark. In Spark 1.0, this is how SQL queries could be run on Spark.

--

```scala
val hiveContext = new HiveContext(sc) // Needs an existing `SparkContext`.

// The `Person` case class provided the necessary schema.
val personsTable: SchemaRDD = hiveContext.createSchemaRDD(persons)

// Registering it makes it available as queriable a table.
hiveContext.registerRDDAsTable(personsTable, "persons")

// And now HiveQL queries can be run.
val numOldJons = hiveContext.hql("""
  |SELECT * FROM persons
  |WHERE age > 30 AND lower(name) LIKE 'jon%'
""".stripMargin.trim).count
```

---

# Project Tungsten

This was a performance optimization initiative by Spark which resulted the introduction of the _Catalyst query optimizer_ for SQL queries over other things.

--

- Spark SQL was introduced. HiveQL support still existed through `HiveContext`, but was not recommended.

--

- `SchemaRDD` was removed and a new `DataFrame` API was introduced.

--

```scala
val sqlContext = new SQLContext(sc) // Needs an existing `SparkContext`.

// A DataFrame can be created using `createDataFrame` and passing an RDD of
// a case class.
val personsDf: DataFrame = sqlContext.createDataFrame(persons)

// A DataFrame can be registered by calling `registerTempTable` on the object.
personsDf.registerTempTable("persons")

// SQL queries can be run using the `sql` function.
val numOldJons = sqlContext.sql("""
  |SELECT * FROM persons
  |WHERE age > 30 AND lower(name) LIKE 'jon%'
""".stripMargin.trim).count
```

---

# DataFrame API

--

It was similar to `SchemaRDD`, except it was faster and supported more features.

--

```scala
// No longer need SQLContext. Can run queries on the DataFrame object.
personsDf.filter("age > 30").filter("lower(name) LIKE 'jon%'").count
```

--

```scala
import org.apache.spark.sql.functions.lower
// Required to use $"colName" which is syntactic sugar for new Column("colName").
import sqlContext.implicits._

// SQL-like DSL enables running typed queries on DataFrames.
personsDf.filter($"age" > 30).filter(lower($"name").startsWith("jon").count
```

--

```scala
// Running non-DSL functions, like `map`, would return an RDD since there is no
// way of knowing what each `Row` object would be mapped into. And all Catalyst
// performance benefits would lost with RDDs.
val info: RDD[String] = personsDf.map { row =>

  // `Row`s are not typed. So we lose on type safety as well.
  val fname = row.getAs[String]("fname")
  val age = row.getAs[Int]("age")

  s"$fname aged $age."
}
```

    </textarea>
    <script src="remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      remark.macros.scale = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
      var slideshow = remark.create();
    </script>
  </body>
</html>
