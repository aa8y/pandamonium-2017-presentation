<!DOCTYPE html>
<html>
  <head>
    <title>Understanding Spark</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      table {
        border-collapse: collapse;
      }
      th, td {
        padding: 5px;
        border: 1px solid black;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Understanding Spark
### by Arun Allamsetty

???
Don't look at these super top-secret notes.
---

# Big Data

--

## What comes to mind?
--

- Yet another buzzword, maybe?

--

- Hadoop?

---

# Big Data

## What comes to mind?

- Yet another buzzword, maybe?

- ~~Hadoop?~~ Probably not.

--

- Spark

--

Why?

---

# Let's start with Hadoop
--

### The Good
--

- Distributed computing was hard and expensive.

???
Communicating with nodes.
Recovering partial work when a node dies.
--

- MapReduce paradigm based on a paper published by Google.

???
Based functional programming map and reduce functions.
---

# map()
--

.center[![:scale 80%](images/MapBase.png)]
---

# map()

.center[![:scale 80%](images/MapResult.png)]
---

# reduce()
--

.center[![:scale 80%](images/MapResult.png)]
---

# reduce()

.center[![:scale 80%](images/ReduceBase.png)]
---

# reduce()

.center[![:scale 80%](images/ReduceResult.png)]
---

# Let's start with Hadoop

### The Good

- Distributed computing was hard and expensive.

- MapReduce paradigm based on a paper published by Google.

- Fault tolerant distributed file system: HDFS

---

# HDFS

.center[![:scale 80%](images/HDFS0.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS1.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS2.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS3.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS4.png)]
---

# Let's start with Hadoop

### The Good

- Distributed computing was hard and expensive.

- MapReduce paradigm based on a paper published by Google.

- Fault tolerant distributed file system: HDFS

- Inexpensive to operate. Can run on commodity hardware.

--

### The bad

--

- Writes to disk after each operation.

???
This was done to provide a checkpoint feature.
--

- Doesn't support in-memory computations.

--

- While MapReduce is easier, it is no SQL (pun not intended).

---

# What about Spark?

???
Takes all the good things from Hadoop and leaves out the bad.
--

- Supports HDFS (or other Hadoop-supported file systems). For example,

--

  - Amazon S3

--

  - Windows Azure Storage Blobs (WASB).

???
Object storage service like S3.
--

- *Does not* write to disk after each operation.

--

  - Uses RDDs instead which can recover from failures.

???
We will see how later.
--

  - Is a lot faster than Hadoop.

--

- Supports in-memory operations.

???
Is one of its major selling points.
--

- Written in Scala. Has API support for Java, Python and R.

???
But we will mainly talk about the Scala API.
---

# RDD (Resilient Distributed Datasets)

--

- Based on a paper by Matei Zaharia.

  - _Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing._

???
Berkeley AMPLab alum and co-founder of Databricks which is primary Spark contributer.
--

They are,

--

- Distributed

???
Data resides on different JVMs on different nodes.
--

- Typed
  - For eg. `RDD[String]`, `RDD[Long]`, `RDD[Person]`, etc.

???
Data in RDDs can be assigned types.
--

- Immutable

???
Data can only be transformed, resulting in a new RDD.
--

- Lazily evaluated

???
No operation is performed an action, which forces execution, is invoked.
Map is a transformation. Reduce is an action.
--

- Fault-tolerant

???
Can recover damaged partitions.
--

- API similar to Scala collections.

???
Let's talk about these more now.
--

  - For eg. `flatMap`, `fold`, `foreach`, `take`, `zipWithIndex` etc.
---

# Creating an RDD

--

- Can be created using a `SparkContext`.

--

  - A `SparkContext` is an entry point into Spark for any Spark application.

--

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")
}
```

???
Scala method parameters can be named.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)
}
```

???
Can be created from a Scala collection.
Method parameters can be mixed and matched.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)

  val rddFromTextFile = sc.textFile("/path/to/text/file")
}
```

--

*Note:* At this point none of these `RDD`s have even been computed.

???
We would have to perform an action which forces execution.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)

  val rddFromTextFile = sc.textFile("/path/to/text/file")

  // We would have to perform an action which forces execution.
  val sum = rddFromCollection.reduce(_ + _) // same as `reduce((x, y) => x + y)`
}
```

---

# Why are RDDs lazy?

--

Let's look at this code snippet.

--

```scala
val nums = sc.parallelize(1 to 100)

// `groupBy` returns an RDD of key-value pairs, `RDD[(Int, Iterable[Int])]` here.
val grouped = nums.groupBy(_ % 3)

// `_1` accesses the first element in a Scala tuple.
// `max` forces computation of an RDD like `reduce`.
val maxKey = grouped.map(_._1).max

// `flatMap` maps over any iterable and flattens all the elements.
val sum = grouped
  .flatMap(_._2)
  .map(_ * maxKey)
  .reduce(_ + _)
```

--

In the Spark UI, we'll see this:

--

.center[![:scale 100%](images/skipped.png)]

---

# RDD dependency (or lineage) graph

--

Spark first creates a DAG of all the operations to be performed. The DAG for the previous `maxKey` looks like this.

--

.center[![:scale 40%](images/maxDag.png)]

---

# RDD dependency (or lineage) graph

The DAG for `sum` however looks like this.

--

.center[![:scale 40%](images/sumDag.png)]

--

Hence, lazy evaluation allows Spark avoid unnecessary recomputation of the same data.

---

# (R)esilience in RDD

--

With this information, let's see how RDDs are fault-tolerant.

--

- If at a stage, a partition is lost due to some reason, Spark knows exactly how to get a new partition at that stage due to the DAG it created in the first place.

--

- Immutability of RDDs ensures that the newly resurrected partition has exactly the same data as before.

---

# RDDs are typed

--

Here's an example.

--

```scala
// Case classes in Scala are a classes with a few helper functions prebuilt
// into them to avoid boilerplate code like in Java.
case class Person(fname: String, lname: String, age: Int)

// :: is a function in a Scala `List` which lets you prepend to a list. `Nil`
// is an empty list. One of the helper function in a case class is `apply`
// which lets you construct objects with function-like syntax.
// `Person("John", "Doe", 25)` is same as `Person.apply("John", "Doe", 25)`.
val persons = sc.parallelize(
  Person("John", "Doe", 38) ::
  Person("Jane", "Doe", 35) ::
  Person("Jon", "Doe", 20) ::
  Persom("Jonathan", "Doe", 31) :: Nil)

val oldPersons = persons.filter(_.age > 30)
val oldJons = oldPersons.filter(_.name.toLowerCase.startsWith("jon"))
val numOldJons = oldJos.count
```

---

# SQL on Spark

--

SQL has been the de facto standard for data analysis since a long time.

--

  - Hadoop lacked in this area. SQL users were not excited to use MapReduce.

--

  - Hive was created to run SQL on Hadoop.

--

  - It was very popular, but was painfully slow.

--

To cater to SQL users, Spark added support for HiveQL which was dubbed Shark. In Spark 1.0, this is how SQL queries could be run on Spark.

--

```scala
val hiveContext = new HiveContext(sc) // Needs an existing `SparkContext`.

// The `Person` case class provided the necessary schema.
val personsTable: SchemaRDD = hiveContext.createSchemaRDD(persons)

// Registering it makes it available as queriable a table.
hiveContext.registerRDDAsTable(personsTable, "persons")

// And now HiveQL queries can be run.
val numOldJons = hiveContext.hql("""
  |SELECT * FROM persons
  |WHERE age > 30 AND lower(name) LIKE 'jon%'
""".stripMargin.trim).count
```

---

# Project Tungsten

This was a performance optimization initiative by Spark which resulted the introduction of the _Catalyst query optimizer_ for SQL queries over other things.

--

- Spark SQL was introduced. HiveQL support still existed through `HiveContext`, but was not recommended.

--

- `SchemaRDD` was removed and a new `DataFrame` API was introduced.

--

```scala
val sqlContext = new SQLContext(sc) // Needs an existing `SparkContext`.

// A DataFrame can be created using `createDataFrame` and passing an RDD of
// a case class.
val personsDf: DataFrame = sqlContext.createDataFrame(persons)

// A DataFrame can be registered by calling `registerTempTable` on the object.
personsDf.registerTempTable("persons")

// SQL queries can be run using the `sql` function.
val numOldJons = sqlContext.sql("""
  |SELECT * FROM persons
  |WHERE age > 30 AND lower(name) LIKE 'jon%'
""".stripMargin.trim).count
```

---

# DataFrame API

--

It was similar to `SchemaRDD`, except it was more performant and supported more features.

--

```scala
// No longer need SQLContext. Can run queries on the DataFrame object.
personsDf.filter("age > 30").filter("lower(name) LIKE 'jon%'").count
```

--

```scala
import org.apache.spark.sql.functions.lower
// Required to use $"colName" which is syntactic sugar for new Column("colName").
import sqlContext.implicits._

// Quasi-SQL DSL to query DataFrames without writing SQL.
personsDf.filter($"age" > 30).filter(lower($"name").startsWith("jon").count
```

--

```scala
// Running non-DSL functions, like `map`, would return an RDD since there is no
// way of knowing what each `Row` object would be mapped into. And all Catalyst
// performance benefits would lost with RDDs.
val info: RDD[String] = personsDf.map { row =>
  // `Row`s are not typed. So we lose on type safety as well.
  val fname = row.getAs[String]("fname")
  val age = row.getAs[Int]("age")

  // Scala string interpolation with s"", similar to JS ``.
  s"$fname aged $age."
}
```

---

# RDD vs DataFrame

| RDD | DataFrame |
|-----|-----------|
| Typed. Error can be caught during compilation. | Untyped. Errors surface at runtime. |

--
| Slower. Cannot benefit from Catalyst optimizer. | Faster. Heavily uses Catalyst optimizer. |

--
| Cannot run SQL. | Can run SQL. |

--
| Fully supports `map`-like operations | `map`-like operations return an RDD. |

--

In the end,

--

- People who heavily invested in RDDs in the beginning were not benefiting from the Catalyst optimizer.

--

- People who wanted the benefits of Catalyst optimizer and wanted to mix and match SQL and MapReduce didn't have a good option.

---

# Dataset API

--

- Spark 1.6 witnessed the release of the Dataset API which _supposedly_ had the best of both worlds.

--

- `Dataset`s were used the Catalyst optimizer and were typed.

--

- They did not support SQL.

--

- So they were *not* the best of both worlds, but just better RDDs.

--

```scala
// Import for Catalyst to understand the typed case classes. This imports
// implicit encoders for generic types (String, Long, etc.) and case classes.
import sqlContext.implicits._

// The `as[Person]` creates a `Dataset[Person]`
val personsDs = sqlContext.createDataFrame(persons).as[Person]

// Operations similar to RDDs can be performed.
val oldJons = personsDs.filter { person =>
  person.age > 30 && person.name.toLowercase.startsWith("jon")
}
val numOldJons = oldJons.count
```

---

# Spark 2.0+ changes

--

- `Dataset`s were *now* the best of both worlds with added support for SQL.

--

- `DataFrame` _API_ was removed.

--

  - `DataFrame`, however, still exists for backward-compatibility but is basically an alias for `Dataset[Row]`.

--

- `SparkSession` has been added as a single point of entry into Spark.

--

  - `SparkContext` and `SQLContext` still exist for backward-compatibility.

--

  - `HiveContext` is still around but has been deprecated.

--

```scala
import org.apache.spark.sql.SparkSession

// `SparkContext` can be accessed using `spark.sparkContext` and
// `SQLContext` using `spark.sqlContext`.
val spark = SparkSession.builder
  .master("local[2]")
  .appName("learningSpark2")
  .config("spark.some.config.option", "some-value")
  .getOrCreate
```

---

# Examples

--

SQL support on `Dataset`s.

--

```scala
// Import implicits from `SparkSession` similar to `SQLContext`.
import spark.implicits._

// Read data as `Dataset`.
val personsDs = spark.createDataset(persons)

// Convert the `Dataset` to `DataFrame` and register as a temporary view.
personsDs.toDf.createOrReplaceTempView("persons")

// Run SQL queries.
val numOldJons = spark.sql("""
  |SELECT * FROM persons
  |WHERE age > 30 AND lower(name) LIKE 'jon%'
""".stripMargin.trim).count
```

--

RDD-like operations.

--

```scala
val oldJons = personsDs.filter { person =>
  person.age > 30 && person.name.toLowercase.startsWith("jon")
}
val numOldJons = oldJons.count
```

---
class: middle, center

# Thank you

    </textarea>
    <script src="remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      remark.macros.scale = function (percentage) {
        const url = this
        return `<img src="${url}" style="width: ${percentage}" />`
      }
      const slideshow = remark.create()
    </script>
  </body>
</html>
