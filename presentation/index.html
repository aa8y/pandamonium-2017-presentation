<!DOCTYPE html>
<html>
  <head>
    <title>Choosing Between Similar But Different Spark APIs</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      table {
        border-collapse: collapse;
      }
      th, td {
        padding: 5px;
        border: 1px solid black;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Choosing Between Similar But Different Spark APIs
### by Arun Allamsetty

???
Don't look at these super top-secret notes.
---

# Big Data

--

## What comes to mind?
--

- Yet another buzzword, maybe?

--

- Hadoop?

---

# Big Data

## What comes to mind?

- Yet another buzzword, maybe?

- ~~Hadoop?~~ Probably not.

--

- Spark

--

Why?

---

# Let's start with Hadoop
--

### The Good
--

- Distributed computing was hard and expensive.

???
Communicating with nodes.
Recovering partial work when a node dies.
--

- MapReduce paradigm based on a paper published by Google.

???
Based functional programming map and reduce functions.
---

# map()
--

.center[![:scale 80%](images/MapBase.png)]
---

# map()

.center[![:scale 80%](images/MapResult.png)]
---

# reduce()
--

.center[![:scale 80%](images/MapResult.png)]
---

# reduce()

.center[![:scale 80%](images/ReduceBase.png)]
---

# reduce()

.center[![:scale 80%](images/ReduceResult.png)]
---

# Let's start with Hadoop

### The Good

- Distributed computing was hard and expensive.

- MapReduce paradigm based on a paper published by Google.

- Fault tolerant distributed file system: HDFS

---

# HDFS

.center[![:scale 80%](images/HDFS0.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS1.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS2.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS3.png)]
---

# HDFS

.center[![:scale 80%](images/HDFS4.png)]
---

# Let's start with Hadoop

### The Good

- Distributed computing was hard and expensive.

- MapReduce paradigm based on a paper published by Google.

- Fault tolerant distributed file system: HDFS

- Inexpensive to operate. Can run on commodity hardware.

--

### The bad

--

- Writes to disk after each operation.

???
This was done to provide a checkpoint feature.
--

- Doesn't support in-memory computations.

--

- While MapReduce is easier, it is no SQL (pun not intended).

---

# What about Spark?

???
Takes all the good things from Hadoop and leaves out the bad.
--

- Supports HDFS (or other Hadoop-supported file systems). For example,

--

  - Amazon S3

--

  - Windows Azure Storage Blobs (WASB).

???
Object storage service like S3.
--

- *Does not* write to disk after each operation.

--

  - Uses RDDs instead which can recover from failures.

???
We will see how later.
--

  - Is a lot faster than Hadoop.

--

- Supports in-memory operations.

???
Is one of its major selling points.
--

- Written in Scala. Has API support for Java, Python and R.

???
But we will mainly talk about the Scala API.
---

# RDD (Resilient Distributed Datasets)

--

- Based on a paper by Matei Zaharia.

  - _Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing._

???
Berkeley AMPLab alum and co-founder of Databricks which is primary Spark contributer.
--

They are,

--

- Distributed

???
Data resides on different JVMs on different nodes.
--

- Typed
  - For eg. `RDD[String]`, `RDD[Long]`, `RDD[Person]`, etc.

???
Data in RDDs can be assigned types.
--

- Immutable

???
Data can only be transformed, resulting in a new RDD.
--

- Lazily evaluated

???
No operation is performed an action, which forces execution, is invoked.
Map is a transformation. Reduce is an action.
--

- Fault-tolerant

???
Can recover damaged partitions.
--

- API similar to Scala collections.

???
Let's talk about these more now.
--

  - For eg. `flatMap`, `fold`, `foreach`, `take`, `zipWithIndex` etc.
---

# Creating an RDD

--

- Can be created using a `SparkContext`.

--

  - A `SparkContext` is an entry point into Spark for any Spark application.

--

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")
}
```

???
Scala method parameters can be named.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)
}
```

???
Can be created from a Scala collection.
Method parameters can be mixed and matched.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)

  val rddFromTextFile = sc.textFile("/path/to/text/file")
}
```

--

*Note:* At this point none of these `RDD`s have even been computed.

???
We would have to perform an action which forces execution.
---

# Creating an RDD

- Can be created using a `SparkContext`.

  - A `SparkContext` is an entry point into Spark for any Spark application.

```scala
import org.apache.spark.SparkContext

object RddExample extends App {
  val sc = new SparkContext(master = "local[2]", appName = "rddExample")

  // `numSlices` defaults to `sc.defaultParallelism` == number of virtual cores.
  val rddFromCollection = sc.parallelize(1 to 100, numSlices = 10)

  val rddFromTextFile = sc.textFile("/path/to/text/file")

  // We would have to perform an action which forces execution.
  val sum = rddFromCollection.reduce(_ + _) // same as `reduce((x, y) => x + y)`
}
```

---

# Why are RDDs lazy?

--

Let's take a simple example.

--

In a list of 100 random numbers find the *max* and *min* *even* and *odd* numbers.

--

```scala
import scala.util.Random

// Generates 100 random numbers between 0 (inclusive) and 1000 (exclusive).
val numsRdd = sc.parallelize(Seq.fill(100)(1000).map(Random.nextInt))
```

---

# Why are RDDs lazy?

Let's take a simple example.

In a list of 100 random numbers find the max and min even and odd number.

```scala
import scala.util.Random

// Generates 100 random numbers between 0 (inclusive) and 1000 (exclusive).
val numsRdd = sc.parallelize(Seq.fill(100)(1000).map(Random.nextInt))

// `groupBy` returns an RDD of key-value pairs, `RDD[(Int, Iterable[Int])]` here.
val grouped = numsRdd.groupBy(_ % 2)
```

---

# Why are RDDs lazy?

Let's take a simple example.

In a list of 100 random numbers find the max and min even and odd number.

```scala
import scala.util.Random

// Generates 100 random numbers between 0 (inclusive) and 1000 (exclusive).
val numsRdd = sc.parallelize(Seq.fill(100)(1000).map(Random.nextInt))

// `groupBy` returns an RDD of key-value pairs, `RDD[(Int, Iterable[Int])]` here.
val grouped = numsRdd.groupBy(_ % 2)

// `collect` is an action forces computation.
val max = grouped.mapValues(_.max).collect
val min = grouped.mapValues(_.min).collect
```

--

In the Spark UI, we'll see this:

--

.center[![:scale 100%](images/skipped.png)]

---

# RDD dependency (or lineage) graph

--

Spark first creates a DAG of all the operations to be performed. The DAG for the `max` operation looks like this.

--

.center[![:scale 100%](images/maxDag.png)]

---

# RDD dependency (or lineage) graph

The DAG for the `min` operation should look the same, right?

--

.center[![:scale 100%](images/minDag.png)]

--

Hence, lazy evaluation allows Spark to avoid recomputations.

---

# A more complex example.

--

Find total student enrollments in all CS courses.

--

```scala
case class User(
  id: Long,
  fname: String,
  lname: String,
  isStudent: Boolean,
  isTeacher: Boolean)
```

---

# A more complex example.

Find total student enrollments in all CS courses.

```scala
case class User(
  id: Long,
  fname: String,
  lname: String,
  isStudent: Boolean,
  isTeacher: Boolean)

case class Course(
  id: Long,
  name: String,
  code: Int,
  department: String,
  `type`: String)
```

---

# A more complex example.

Find total student enrollments in all CS courses.

```scala
case class User(
  id: Long,
  fname: String,
  lname: String,
  isStudent: Boolean,
  isTeacher: Boolean)

case class Course(
  id: Long,
  name: String,
  code: Int,
  department: String,
  `type`: String)

case class Enrollment(
  id: Long,
  userId: Long,
  courseId: Long,
  isArchived: Option[Boolean] = None)
```

---

# A more complex example.

Some sample data.

```scala
val users = sc.parallelize(
  User(1, "Teacher", "Doe", false, true) ::
  User(2, "Another Teacher", "Doe", false, true) ::
  User(3, "TA", "Doe", true, true) ::
  User(4, "Lazy TA", "Doe", true, true) ::
  User(5, "Student", "Doe", true, false) ::
  User(6, "Another Student", "Doe", true, false) ::
  User(7, "Yet Another Student", "Doe", true, false) :: Nil)
```

---

# A more complex example.

Some sample data.

```scala
val users = sc.parallelize(
  User(1, "Teacher", "Doe", false, true) ::
  User(2, "Another Teacher", "Doe", false, true) ::
  User(3, "TA", "Doe", true, true) ::
  User(4, "Lazy TA", "Doe", true, true) ::
  User(5, "Student", "Doe", true, false) ::
  User(6, "Another Student", "Doe", true, false) ::
  User(7, "Yet Another Student", "Doe", true, false) :: Nil)

val courses = sc.parallelize(
  Course(1, "Algorithms", 100, "CS", "Core") ::
  Course(2, "Dissecting Frogs", 200, "Bio", "Elective") ::
  Course(3, "Global Warming: That thing invented by China", 100, "Bio",
    "NowAnElective") ::
  Course(4, "Computer Networks: Hacker stuff!", 300, "CS", "Elective") :: Nil)
```

---

# A more complex example.

Some sample data.

```scala
val users = sc.parallelize(
  User(1, "Teacher", "Doe", false, true) ::
  User(2, "Another Teacher", "Doe", false, true) ::
  User(3, "TA", "Doe", true, true) ::
  User(4, "Lazy TA", "Doe", true, true) ::
  User(5, "Student", "Doe", true, false) ::
  User(6, "Another Student", "Doe", true, false) ::
  User(7, "Yet Another Student", "Doe", true, false) :: Nil)

val courses = sc.parallelize(
  Course(1, "Algorithms", 100, "CS", "Core") ::
  Course(2, "Dissecting Frogs", 200, "Bio", "Elective") ::
  Course(3, "Global Warming: That thing invented by China", 100, "Bio",
    "NowAnElective") ::
  Course(4, "Computer Networks: Hacker stuff!", 300, "CS", "Elective") :: Nil)

val archivedEnrollments = sc.parallelize(
  Enrollment(id = 11, userId = 1, courseId = 1, isArchived = Some(true)) ::
  Enrollment(id = 12, userId = 3, courseId = 1, isArchived = Some(true)) ::
  Enrollment(id = 13, userId = 5, courseId = 1, isArchived = Some(true)) ::
  Enrollment(id = 14, userId = 2, courseId = 2, isArchived = Some(true)) ::
  Enrollment(id = 15, userId = 6, courseId = 2, isArchived = Some(true)) :: Nil)
```

---

# A more complex example.

```scala
val currentEnrollments = sc.parallelize(
  Enrollment(id = 21, userId = 1, courseId = 1) ::
  Enrollment(id = 22, userId = 3, courseId = 1) ::
  Enrollment(id = 23, userId = 6, courseId = 1) ::
  Enrollment(id = 24, userId = 4, courseId = 4) ::
  Enrollment(id = 25, userId = 7, courseId = 4) ::
  Enrollment(id = 26, userId = 2, courseId = 2) ::
  Enrollment(id = 27, userId = 2, courseId = 3) ::
  Enrollment(id = 26, userId = 5, courseId = 2) ::
  Enrollment(id = 27, userId = 6, courseId = 3) :: Nil)
```

---

# A more complex example.

```scala
val currentEnrollments = sc.parallelize(
  Enrollment(id = 21, userId = 1, courseId = 1) ::
  Enrollment(id = 22, userId = 3, courseId = 1) ::
  Enrollment(id = 23, userId = 6, courseId = 1) ::
  Enrollment(id = 24, userId = 4, courseId = 4) ::
  Enrollment(id = 25, userId = 7, courseId = 4) ::
  Enrollment(id = 26, userId = 2, courseId = 2) ::
  Enrollment(id = 27, userId = 2, courseId = 3) ::
  Enrollment(id = 26, userId = 5, courseId = 2) ::
  Enrollment(id = 27, userId = 6, courseId = 3) :: Nil)

val csCourses = courses.filter(_.department == "CS")
```

---

# A more complex example.

```scala
val currentEnrollments = sc.parallelize(
  Enrollment(id = 21, userId = 1, courseId = 1) ::
  Enrollment(id = 22, userId = 3, courseId = 1) ::
  Enrollment(id = 23, userId = 6, courseId = 1) ::
  Enrollment(id = 24, userId = 4, courseId = 4) ::
  Enrollment(id = 25, userId = 7, courseId = 4) ::
  Enrollment(id = 26, userId = 2, courseId = 2) ::
  Enrollment(id = 27, userId = 2, courseId = 3) ::
  Enrollment(id = 26, userId = 5, courseId = 2) ::
  Enrollment(id = 27, userId = 6, courseId = 3) :: Nil)

val csCourses = courses.filter(_.department == "CS")

val students = users.filter(_.isStudent).map { u => (u.id, u) }
val studentsOnly = users.filter { u => u.isStudent && !u.isTeacher }
  .map { s => (s.id, s) }
```

---

# A more complex example.

```scala
val currentEnrollments = sc.parallelize(
  Enrollment(id = 21, userId = 1, courseId = 1) ::
  Enrollment(id = 22, userId = 3, courseId = 1) ::
  Enrollment(id = 23, userId = 6, courseId = 1) ::
  Enrollment(id = 24, userId = 4, courseId = 4) ::
  Enrollment(id = 25, userId = 7, courseId = 4) ::
  Enrollment(id = 26, userId = 2, courseId = 2) ::
  Enrollment(id = 27, userId = 2, courseId = 3) ::
  Enrollment(id = 26, userId = 5, courseId = 2) ::
  Enrollment(id = 27, userId = 6, courseId = 3) :: Nil)

val csCourses = courses.filter(_.department == "CS")

val students = users.filter(_.isStudent).map { u => (u.id, u) }
val studentsOnly = users.filter { u => u.isStudent && !u.isTeacher }
  .map { s => (s.id, s) }

val archivedCsEnrollments = archivedEnrollments.map { e => (e.userId, e) }
  .join(students)
val currentCsEnrollments = currentEnrollments.map { e => (e.userId, e) }
  .join(studentsOnly)
val allCurrentCsEnrollments = currentEnrollments.map { e => (e.userId, e) }
  .join(students)
```

---

# A more complex example.

```scala
val currentEnrollments = sc.parallelize(
  Enrollment(id = 21, userId = 1, courseId = 1) ::
  Enrollment(id = 22, userId = 3, courseId = 1) ::
  Enrollment(id = 23, userId = 6, courseId = 1) ::
  Enrollment(id = 24, userId = 4, courseId = 4) ::
  Enrollment(id = 25, userId = 7, courseId = 4) ::
  Enrollment(id = 26, userId = 2, courseId = 2) ::
  Enrollment(id = 27, userId = 2, courseId = 3) ::
  Enrollment(id = 26, userId = 5, courseId = 2) ::
  Enrollment(id = 27, userId = 6, courseId = 3) :: Nil)

val csCourses = courses.filter(_.department == "CS")

val students = users.filter(_.isStudent).map { u => (u.id, u) }
val studentsOnly = users.filter { u => u.isStudent && !u.isTeacher }
  .map { s => (s.id, s) }

val archivedCsEnrollments = archivedEnrollments.map { e => (e.userId, e) }
  .join(students)
val currentCsEnrollments = currentEnrollments.map { e => (e.userId, e) }
  .join(studentsOnly)
val allCurrentCsEnrollments = currentEnrollments.map { e => (e.userId, e) }
  .join(students)

val csEnrollments = archivedCsEnrollments.union(currentCsEnrollments).count
val allCsEnrollments = archivedCsEnrollments.union(allCurrentCsEnrollments).count
```

---

# A more complex example.

Again we have a few skipped tasks.

--

.center[![:scale 100%](images/complexSkipped.png)]

---

# A more complex example.

.center[![:scale 95%](images/complexStudents.png)]

---

# A more complex example.

.center[![:scale 95%](images/complexAllStudents.png)]

---

# (R)esilience in RDD

--

These lineage graphs are also provide fault-tolerance.

--

- Tells Spark exactly how to recover a lost partition at any stage.

--

- Immutability of RDDs ensures that the newly resurrected partition has exactly the same data as before.

---

# SQL on Spark

--

SQL has been the de facto standard for data analysis since a long time.

--

  - Hadoop did not have built-in SQL support.

???
SQL users were not excited to use MapReduce.
--

  - Hive was created to run SQL on Hadoop. Popular, but was painfully slow.

--

Spark shipped with HiveQL support (dubbed Shark).

---

# SQL on Spark

Example for Spark 1.0.

--

```scala
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc) // Needs an existing `SparkContext`.

// Re-using existing RDDs from a previous example.
val usersTable = hiveContext.createSchemaRDD(users)
val coursesTable = hiveContext.createSchemaRDD(courses)
val archivedEnrollmentsTable = hiveContext.createSchemaRDD(archivedEnrollments)
val enrollmentsTable = hiveContext.createSchemaRDD(enrollments)

// Registering the SchemaRDD makes it available for queries.
hiveContext.registerRDDAsTable(usersTable, "users")
hiveContext.registerRDDAsTable(coursesTable, "courses")
hiveContext.registerRDDAsTable(archivedEnrollmentsTable, "archivedEnrollments")
hiveContext.registerRDDAsTable(enrollmentsTable, "enrollments")

// And now HiveQL queries can be run.
val csEnrollments = hiveContext.hql("""
  |SELECT * FROM (
  |  SELECT * FROM archivedEnrollments WHERE department = "CS" UNION
  |  SELECT * FROM enrollments WHERE department = "CS"
  |) e INNER JOIN (
  |  SELECT * FROM users WHERE isStudent
  |) s ON e.studentId = s.id
""".stripMargin.trim).count
```

---

# Conclusions so far

--

- Use `RDD`s if you want to use Scala (case) classes.

--

- Use `HiveContext` with `SchemaRDD` if you want to run SQL queries.

--

- `SchemaRDD`s are not typesafe. So no compile-time warnings.

---

# Project Tungsten

--

_Catalyst query optimizer_ for SQL queries introduced.

???
Performance optimization initiative by Spark which introduced Catalyst query optimizer among other things.
--

- Spark SQL was introduced (to be used with `SQLContext`).

--

- HiveQL support still existed through `HiveContext`.

???
It was to be removed in future major releases.
--

- `SchemaRDD` was removed and a new `DataFrame` API was introduced.

--

The API was _similar_ to what it was before, except more performant.
```scala
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc) // Needs an existing `SparkContext`.

// A `DataFrame` can be created using `createDataFrame` vs `createSchemaRDD`
val usersDf = sqlContext.createDataFrame(users)
...

// A DataFrame can be registered by calling `registerTempTable` on the object
usersDf.registerTempTable("users")  // vs `registerRDDAsTable` on the context.
...

// SQL queries can be run using the `sql` function on a `SQLContext` vs `hql`.
val csEnrollments = sqlContext.sql(...).count
```

---

# DataFrame API

--

```scala
// Can run queries on the DataFrame object using RDD-like methods.
coursesDf.filter("department = 'CS'")
```

```scala
// Required to use $"colName" which is syntactic sugar for `new Column("colName")`.
import sqlContext.implicits._
// `lit` creates Column with literal value.
import org.apache.spark.sql.functions.lit
usersDf.filter($"isStudent" === lit(true))
```

--

```scala
// Quasi-SQL DSL to query DataFrames without writing SQL.
val enrollmentsDf = currentEnrollmentsDf.unionAll(archivedEnrollmentsDf)
```

--

```scala
// `map`, `flatMap` and similar methods, would return an RDD.
val sortableNames: RDD[String] = usersDf.map { row =>
  // `Row`s are not typed. So we lose on type safety as well.
  val fname = row.getAs[String]("fname")
  val lname = row.getAs[String]("lname")
  s"$lname, $fname"
}
```

---

# Conclusions so far

--

- Use `RDD`s if you want to use Scala (case) classes.

--

- Use ~~`HiveContext`~~ `SQLContext` with ~~`SchemaRDD`s~~ `DataFrame`s if you want to run SQL queries.

--

- ~~`SchemaRDD`s~~ `DataFrame`s are (also) not typesafe. So no compile-time warnings.

--

- `DataFrame`s are more performant than `RDD`s.

--

- `map`ping over a `DataFrame` returns an `RDD`.

???
- People who invested in RDDs could not benefit from Catalyst.
- People who wanted the benefits of Catalyst optimizer lost on type-safety.
---

# Dataset API

--

Spark 1.6 released with `Dataset` API.

--

- Typesafe way to benefit from the Catalyst optimizer.

--

- Did not support SQL.

--

- `map`-like methods did not return `RDD`s anymore.

--

```scala
org.apache.spark.sql.Dataset

// Import for Catalyst to understand the typed case classes. This imports
// implicit encoders for generic types (String, Long, etc.) and case classes.
import sqlContext.implicits._

// The `as[User]` creates a `Dataset[User]`
val usersDs = sqlContext.createDataFrame(users).as[User]

// Operations similar to RDDs can be performed without losing on performance.
val oldJons: Dataset[String] = personsDs.filter(_.isStudent).map { s =>
  s"${s.lname}, ${s.fname}"
}
```

---

# Conclusions so far

--

- Use ~~`RDD`s~~ `Dataset`s if you want to use Scala (case) classes.

--

- Use ~~`HiveContext`~~ `SQLContext` with ~~`SchemaRDD`s~~ `DataFrame`s if you want to run SQL queries.

- ~~`SchemaRDD`s~~ `DataFrame`s are (also) not typesafe. So no compile-time warnings.

- `DataFrame`s are more performant than `RDD`s.

- `map`ping over a `DataFrame` returns an `RDD`.

???
No changes here.
--

- `map`ping over a `Dataset` returns a `Dataset`.

--

- `Dataset`s are as performant as `DataFrame`s along with being typesafe.

???
So no reason to use DF unless SQL. Avoid RDDs.
---

# Spark 2.0+ changes

--

- `DataFrame` _API_ has been removed.

--

  - `DataFrame`, however, still exists for backward-compatibility.

  - It is basically an alias for `Dataset[Row]`.

--

- Hence, `Dataset`s *now* support SQL.

--

- `SparkSession` has been added as a single point of entry into Spark.

--

  - `SparkContext` and `SQLContext` still exist for backward-compatibility.

--

  - `HiveContext` is still around but has been deprecated.

--

```scala
import org.apache.spark.sql.SparkSession

// `SparkContext` can be accessed using `spark.sparkContext` and
// `SQLContext` using `spark.sqlContext`.
val spark = SparkSession.builder
  .master("local[2]")
  .appName("learningSpark2")
  .config("spark.some.config.option", "some-value")
  .getOrCreate
```

---

# SQL support example

--

```scala
// Import implicits from `SparkSession` similar to `SQLContext`.
import spark.implicits._

// `Dataset` can be created using `createDataset`
val usersDs = spark.createDataset(users) // vs `createDataFrame(users).as[User]`.

// Convert the `Dataset` to `DataFrame` and register as a temporary view.
usersDs.toDf.createOrReplaceTempView("users")

// SQL queries can be run using the `sql` function on a `SparkSession`
val csEnrollments = spark.sql(...).count  // vs `SQLContext`.
```

---

# Final Conclusions

--

- Use ~~`RDD`s~~ `Dataset`s if you want to use Scala (case) classes.

--

- Use ~~`HiveContext`~~ ~~`SQLContext`~~ `SparkSession` with ~~`SchemaRDD`s~~ `DataFrame`s/`Dataset`s if you want to run SQL queries.

--

- ~~`SchemaRDD`s~~ `DataFrame`s are _still_ not typesafe being an alias for `Dataset[Row]`.

--

- `DataFrame`s are more performant than `RDD`s.

--

- `map`ping over a `DataFrame` returns ~~an `RDD`~~ a `Dataset`.

--

- `map`ping over a `Dataset` returns a `Dataset`.

--

#### So if you're using Spark 2, try using `Dataset`s as much as possible.

---
class: middle, center

# Thank you

    </textarea>
    <script src="remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      remark.macros.scale = function (percentage) {
        const url = this
        return `<img src="${url}" style="width: ${percentage}" />`
      }
      const slideshow = remark.create()
    </script>
  </body>
</html>
